{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enzynet import constants\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "#from keras.layers import advanced_activations\n",
    "from keras import models\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (302113022.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    flags_dict = FLAGS._flags()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'mode_dataset' is defined twice. First from /Users/harishwar/anaconda3/envs/newenv/lib/python3.9/site-packages/ipykernel_launcher.py, Second from /Users/harishwar/anaconda3/envs/newenv/lib/python3.9/site-packages/ipykernel_launcher.py.  Description from first occurrence: <full|reduced>: Version of the dataset to use. \"full\" represents the entire dataset whereas \"reduced\" denotes just a 10% fixed random subset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m FLAGS \u001b[38;5;241m=\u001b[39m flags\u001b[38;5;241m.\u001b[39mFLAGS\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Main parameters.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFINE_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmode_dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                  \u001b[49m\u001b[43menum_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mhelp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVersion of the dataset to use. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m represents the \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentire dataset whereas \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreduced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m denotes just a 10\u001b[39;49m\u001b[38;5;132;43;01m% f\u001b[39;49;00m\u001b[38;5;124;43mixed \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom subset.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m flags\u001b[38;5;241m.\u001b[39mDEFINE_enum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode_run\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, enum_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     31\u001b[0m                   help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhether to run the training or testing portion of the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Volume parameters.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/newenv/lib/python3.9/site-packages/absl/flags/_defines.py:469\u001b[0m, in \u001b[0;36mDEFINE_enum\u001b[0;34m(name, default, enum_values, help, flag_values, module_name, required, **args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDEFINE_enum\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=invalid-name,redefined-builtin\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     name,\n\u001b[1;32m    440\u001b[0m     default,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m     required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Registers a flag whose value can be any string from enum_values.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m  Instead of a string enum, prefer `DEFINE_enum_class`, which allows\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    a handle to defined flag.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFINE_flag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_flag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumFlag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/newenv/lib/python3.9/site-packages/absl/flags/_defines.py:136\u001b[0m, in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Copying the reference to flag_values prevents pychecker warnings.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m fv \u001b[38;5;241m=\u001b[39m flag_values\n\u001b[0;32m--> 136\u001b[0m fv[flag\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m flag\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Tell flag_values who's defining the flag.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name:\n",
      "File \u001b[0;32m~/anaconda3/envs/newenv/lib/python3.9/site-packages/absl/flags/_flagvalues.py:432\u001b[0m, in \u001b[0;36mFlagValues.__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_module_defining_flag(name) \u001b[38;5;241m==\u001b[39m module_name \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    427\u001b[0m       \u001b[38;5;28mid\u001b[39m(module) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_module_id_defining_flag(name)):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# If the flag has already been defined by a module with the same name,\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# but a different ID, we can stop here because it indicates that the\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# module is simply being imported a subsequent time.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _exceptions\u001b[38;5;241m.\u001b[39mDuplicateFlagError\u001b[38;5;241m.\u001b[39mfrom_flag(name, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    433\u001b[0m short_name \u001b[38;5;241m=\u001b[39m flag\u001b[38;5;241m.\u001b[39mshort_name\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# modules if it's not registered.\u001b[39;00m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'mode_dataset' is defined twice. First from /Users/harishwar/anaconda3/envs/newenv/lib/python3.9/site-packages/ipykernel_launcher.py, Second from /Users/harishwar/anaconda3/envs/newenv/lib/python3.9/site-packages/ipykernel_launcher.py.  Description from first occurrence: <full|reduced>: Version of the dataset to use. \"full\" represents the entire dataset whereas \"reduced\" denotes just a 10% fixed random subset."
     ]
    }
   ],
   "source": [
    "\"\"\"Run the EnzyNet architecture.\"\"\"\n",
    "\n",
    "# Authors: Afshine Amidi <lastname@mit.edu>\n",
    "#          Shervine Amidi <firstname@stanford.edu>\n",
    "\n",
    "# MIT License\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import os\n",
    "\n",
    "from enzynet import constants\n",
    "from enzynet import keras_utils\n",
    "from enzynet import models\n",
    "from enzynet import tools\n",
    "from enzynet import volume\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Main parameters.\n",
    "flags.DEFINE_enum('mode_dataset', default='full',\n",
    "                  enum_values=['full', 'reduced'],\n",
    "                  help='Version of the dataset to use. \"full\" represents the '\n",
    "                  'entire dataset whereas \"reduced\" denotes just a 10% fixed '\n",
    "                  'random subset.')\n",
    "flags.DEFINE_enum('mode_run', default='test', enum_values=['train', 'test'],\n",
    "                  help='Whether to run the training or testing portion of the '\n",
    "                  'code.')\n",
    "\n",
    "# Volume parameters.\n",
    "flags.DEFINE_list('weights', default=[], help='Weights to be used along with '\n",
    "                  'coordinates to characterize PDB volumes. Each weight type '\n",
    "                  'specified here adds an additional channel.')\n",
    "flags.DEFINE_bool('scaling_weights', default=True, help='If set to True, '\n",
    "                  'weight values are scaled to have a maximum magnitude of 1. '\n",
    "                  'Note: this parameter is ignored if no weights are '\n",
    "                  'specified.')\n",
    "flags.DEFINE_integer('p', lower_bound=0, default=0, help='Number of '\n",
    "                     'interpolated points between two consecutive represented '\n",
    "                     'atoms. This parameter is used for finer grid '\n",
    "                     'representations in order to draw lines between '\n",
    "                     'consecutive points.')\n",
    "flags.DEFINE_integer('v_size', lower_bound=1, default=32, help='Size of each '\n",
    "                     'dimension of the grid where enzymes are represented. The '\n",
    "                     'paper used a value of 32 but higher values such as 64 or '\n",
    "                     '96 (cf. Figure 2 of the paper) could prove to carry '\n",
    "                     'finer, more useful structural information. In this case, '\n",
    "                     'please note that the architecture would also likely need '\n",
    "                     'an upgrade in order to see performance gains.')\n",
    "flags.DEFINE_float('max_radius', lower_bound=0.1, default=40, help='Maximum '\n",
    "                   'enzyme radius (in angstroms) that entirely fits in the '\n",
    "                   'volume. A higher value means more information is included '\n",
    "                   'in the grid at a coarser resolution. Dataset statistics '\n",
    "                   'are provided in Figure 4 of the paper.')\n",
    "flags.DEFINE_bool('shuffle', default=True, help='Whether to shuffle the order '\n",
    "                  'of dataset exploration in-between training epochs.')\n",
    "flags.DEFINE_bool('noise_treatment', default=False, help='Whether to remove '\n",
    "                  'isolated atoms from the enzyme volume. This is makes the '\n",
    "                  'structure visually more coherent at fine grid sizes (> 64) '\n",
    "                  'but is an almost no-op at lower resoltions (e.g. 32).')\n",
    "flags.DEFINE_float('flip_probability', lower_bound=0.0, upper_bound=1.0,\n",
    "                   default=0.2, help='Probability of flipping the enzyme with '\n",
    "                   'respect to any axis. Used as an augmentation technique '\n",
    "                   'during training.')\n",
    "\n",
    "# Training parameters.\n",
    "flags.DEFINE_integer('batch_size', lower_bound=1, default=32, help='Training '\n",
    "                     'and validation batch size.')\n",
    "flags.DEFINE_integer('max_epochs', lower_bound=1, default=200, help='Number of '\n",
    "                     'training epochs.')\n",
    "flags.DEFINE_enum('mode_weights', default='unbalanced',\n",
    "                  enum_values=['unbalanced', 'balanced'],\n",
    "                  help='Characterization of the strategy for mistake '\n",
    "                  'penalization in the calculation of the loss.')\n",
    "flags.DEFINE_integer('period_checkpoint', lower_bound=1, default=50,\n",
    "                     help='Epoch frequency at which model weights are saved '\n",
    "                     'during the training process.')\n",
    "\n",
    "# Testing parameters.\n",
    "flags.DEFINE_enum('voting_type', default='probabilities',\n",
    "                  enum_values=['classes', 'probabilities'],\n",
    "                  help='Determines how voting decisions are merged. '\n",
    "                  '\"probabilities\" is used when the predicted class is the '\n",
    "                  'argmax of the sum of probabilities, \"classes\" when the '\n",
    "                  'argmax operates on the sum of predicted classes.')\n",
    "flags.DEFINE_list('augmentation', default=['None', 'flips', 'weighted_flips'],\n",
    "                  help='Denotes the augmentation techniques used at testing '\n",
    "                  'time.')\n",
    "\n",
    "# Miscellaneous.\n",
    "LOSS_TYPE_TO_NAME = {\n",
    "    'unbalanced': 'enzynet_uniform',\n",
    "    'balanced': 'enzynet_adapted',\n",
    "}\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    ##---------------------------- Dataset -----------------------------------##\n",
    "    # Load dictionary of labels.\n",
    "    DICTIONARY = tools.read_dict(\n",
    "        os.path.join(constants.DATASETS_DIR, 'dataset_single.csv'),\n",
    "        value_type=constants.ValueType.INT)\n",
    "\n",
    "    # Load partitions.\n",
    "    if FLAGS.mode_dataset == 'full':\n",
    "        partition = tools.read_dict(\n",
    "            os.path.join(constants.DATASETS_DIR, 'partition_single.csv'),\n",
    "            value_type=constants.ValueType.LIST_STRING)\n",
    "    elif FLAGS.mode_dataset == 'reduced':\n",
    "        partition = tools.read_dict(\n",
    "            os.path.join(constants.DATASETS_DIR, 'partition_single_red.csv'),\n",
    "            value_type=constants.ValueType.LIST_STRING)\n",
    "\n",
    "    # Final computations.\n",
    "    partition['train'] = partition['train'] + partition['validation']\n",
    "    partition['validation'] = partition['test']\n",
    "\n",
    "    # Get class weights and run type.\n",
    "    class_weights = tools.get_class_weights(DICTIONARY, partition['train'],\n",
    "                                            mode=FLAGS.mode_weights)\n",
    "    run_type = LOSS_TYPE_TO_NAME[FLAGS.mode_weights]\n",
    "\n",
    "    # Training generator.\n",
    "    training_generator = volume.VolumeDataGenerator(\n",
    "        list_enzymes=partition['train'],\n",
    "        labels=DICTIONARY,\n",
    "        v_size=FLAGS.v_size,\n",
    "        flips=(FLAGS.flip_probability,) * constants.N_DIMENSIONS,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        shuffle=FLAGS.shuffle,\n",
    "        p=FLAGS.p,\n",
    "        max_radius=FLAGS.max_radius,\n",
    "        noise_treatment=FLAGS.noise_treatment,\n",
    "        weights=FLAGS.weights,\n",
    "        scaling_weights=FLAGS.scaling_weights)\n",
    "\n",
    "    # Validation generator.\n",
    "    validation_generator = volume.VolumeDataGenerator(\n",
    "        list_enzymes=partition['validation'],\n",
    "        labels=DICTIONARY,\n",
    "        v_size=FLAGS.v_size,\n",
    "        flips=(0,) * constants.N_DIMENSIONS,  # No flip.\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        shuffle=False,  # Validate with fixed set.\n",
    "        p=FLAGS.p,\n",
    "        max_radius=FLAGS.max_radius,\n",
    "        noise_treatment=FLAGS.noise_treatment,\n",
    "        weights=FLAGS.weights,\n",
    "        scaling_weights=FLAGS.scaling_weights)\n",
    "\n",
    "    # Check if data has been precomputed.\n",
    "    training_generator.check_precomputed()\n",
    "\n",
    "    ##--------------------------- Testing ------------------------------------##\n",
    "    # Voting object.\n",
    "    predictions = keras_utils.Voting(\n",
    "        list_enzymes=partition['test'],\n",
    "        labels=DICTIONARY,\n",
    "        voting_type=FLAGS.voting_type,\n",
    "        v_size=FLAGS.v_size,\n",
    "        augmentation=FLAGS.augmentation,\n",
    "        p=FLAGS.p,\n",
    "        max_radius=FLAGS.max_radius,\n",
    "        noise_treatment=FLAGS.noise_treatment,\n",
    "        weights=FLAGS.weights,\n",
    "        scaling_weights=FLAGS.scaling_weights)\n",
    "\n",
    "    ##---------------------------- Model -------------------------------------##\n",
    "    # Retrieve EnzyNet model architecture.\n",
    "    model = models.enzynet(FLAGS.v_size, n_channels=1+len(FLAGS.weights))\n",
    "\n",
    "    # Track accuracy and loss in real-time.\n",
    "    history = keras_utils.MetricsHistory(saving_path=run_type + '.csv')\n",
    "\n",
    "    # Checkpoints.\n",
    "    checkpoints = callbacks.ModelCheckpoint(\n",
    "        os.path.join(\n",
    "            constants.CHECKPOINTS_DIR, f'{run_type}_{{epoch:02d}}.hd5f'),\n",
    "        save_weights_only=True,\n",
    "        period=FLAGS.period_checkpoint)\n",
    "\n",
    "    if FLAGS.mode_run == 'train':\n",
    "        # Compile.\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001, decay=0.00016667),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train.\n",
    "        model.fit_generator(generator=training_generator,\n",
    "                            epochs=FLAGS.max_epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[history, checkpoints],\n",
    "                            class_weight=class_weights,\n",
    "                            use_multiprocessing=True,\n",
    "                            workers=8,\n",
    "                            max_queue_size=30)\n",
    "\n",
    "    if FLAGS.mode_run == 'test':\n",
    "        # Load weights.\n",
    "        weights_path = os.path.join(constants.CHECKPOINTS_DIR,\n",
    "                                    f'{run_type}_{FLAGS.max_epochs:02d}.hd5f')\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    # Predict.\n",
    "    predictions.predict(model)\n",
    "\n",
    "    # Compute indicators.\n",
    "    predictions.get_assessment()\n",
    "\n",
    "    # Clear session.\n",
    "    K.clear_session()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
